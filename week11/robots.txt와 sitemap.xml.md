# robots.txt

**웹 사이트 검색엔진로봇들의 접근을 조절/제어**하며,
로봇들에게 **웹 사이트의 사이트맵이 어디 있는지** 알려주는 역할을 한다.

> 주의: 여기서의 사이트 맵은 수집 로봇을 위한 XML 사이트 맵이며, 웹 사이트의 메뉴 전체를 보여주는 사이트 맵을 의미하는 것은 아니다.

### robots.txt 파일을 설정하지 않으면?

구글, 네이버 등의 각종 검색엔진봇들이
**웹 사이트에서 찾을 수 있는 모든 정보를 크롤링**하여 검색엔진 결과에 노출시킨다.

=> 웹 사이트 내부의 특정 페이지가 검색엔진에 노출되지 않았으면 할 때 robots.txt를 사용한다.

<img src="https://www.twinword.co.kr/wp-content/uploads/sites/6/2017/09/6-1-1024x371.png" width="500" />

## 설정

html이 아닌 **일반 텍스트파일로 작성**되며, **사이트의 root 디렉토리에 위치**한다.
(다른 곳에 올리면 검색 로봇이 찾을 수 없다)

또한 **누구에게나 공개된 파일**로, **특정 웹사이트 URL 뒤에 /robots.txt 를 입력**하면
다른 웹사이트가 어떻게 robots.txt 파일을 적용시키고 있는지 확인할 수 있다.

#### 예: [코딩애플 사이트](https://codingapple.com/)의 robots.txt

<img src="https://images.velog.io/images/yena1025/post/17af1eb8-5ed1-45f7-9781-4935abc5a1b2/image.png" width="400" />

---

만약 robots.txt를 다음과 같이 설정하면,

```jsx
User-agent: *
Disallow: /
```

**모든 웹 사이트 컨텐츠에 대한
모든 웹 크롤러의 접근을 차단**하는 것을 의미한다.

그러나 웹사이트의 모든 콘텐츠를 차단시키면 SEO에 별 의미가 없으므로
**가능한 한 모든 콘텐츠를 허용하되 몇 가지 컨텐츠만 차단**하는 방식을 사용하게 된다.

#### 추가 예시

```jsx
User-agent: Yeti
Disallow: /hello/
```

이렇게 설정하면
웹 사이트의 모든 컨텐츠에 대해 ** 네이버 검색봇(Yeti)의 크롤링을 허용**하되,
**/hello/ 디렉토리 안의 페이지에 대한 접근은 차단**하게 된다.

> 대표적인 검색봇: 구글(Googlebot), 네이버(Yeti), 빙(Bingbot), 야후(Slurp)

## SEO에 주는 영향

robots.txt는 언뜻 보면 **웹 크롤러의 접근을 막는 역할**을 하는 것처럼 보여서
SEO(검색엔진 최적화)에 어떤 도움을 주는지 의문이 들 수 있다.

하지만, 만약 웹 사이트 내에 **똑같은 콘텐츠를 가진 웹 페이지가 여러 개 있을 때**
웹 크롤러가 사이트의 모든 페이지를 읽어간다면 이는 오히려 검색엔진 최적화에 부정적인 영향을 줄 수 있다.

특히 구글의 경우 **중복 컨텐츠에 대해 페널티**를 주기 때문에(지나치게 recursive하면 광고로 판단하여 blocking)
만약 **웹사이트 내에 중복적인 콘텐츠가 있을 경우 robots.txt를 통해 적절히 제어**해주는 것이 좋다.

또한, robots.txt는 **웹사이트의 사이트맵 위치를 포함**하기 때문에
검색봇들에게 **어디에서 웹사이트 정보를 가져가야 할지 알려주는 역할**도 한다.

# Sitemap.xml

**웹 사이트 내의 모든 페이지 목록을 나열한 파일**로,
**책의 목차**와 같은 역할을 한다.

sitemap을 제출하면, **일반적인 크롤링 과정에서 쉽게 발견되지 않는 웹페이지도
잘 크롤링되고 색인**될 수 있게 해준다.

<img src="https://www.twinword.co.kr/wp-content/uploads/sites/6/2017/09/7-1-1024x366-1.png" width="600" />

## 설정

sitemap.xml 파일은 robots.txt 파일과 달리,
**꼭 root 디렉토리에 위치하지 않아도 된다.**

하지만 많은 웹 사이트들이 **sitemap.xml 파일도 robots.txt와 마찬가지로 루트 폴더에 업로드**하기 때문에,
다른 웹 사이트들의 sitemap.xml을 참고하고 싶다면 **웹 사이트의 URL 뒤에 /sitemap.xml 을 입력**하여 볼 수 있다.

#### 예: [드림코딩 아카데미 사이트](https://academy.dream-coding.com/)의 sitemap.xml

<img src="https://images.velog.io/images/yena1025/post/20ae56ea-f9ff-4014-830e-c2f2cf181e52/image.png" width="600" />

---

sitemap.xml은 전 세계적으로 약속된 정해진 양식으로 제작되어야 한다.

> sitemap.xml에 관한 샘플 포맷, xml 태그에 대한 자세한 설명 참고하기: [Sitemap.org 사이트](https://www.sitemaps.org/index.html)

최근에는 사이트맵을 무료로 생성해주는 온라인 생성기도 있어서 이러한 사이트들을 활용하여 빠르게 sitemap을 제작할 수 있다.

> 관련 글: [사이트맵을 쉽게 만드는 3가지 방법과 제출하는 방법](https://www.twinword.co.kr/blog/3-different-ways-to-generate-and-submit-sitemap/)

## SEO에 주는 영향

파일 생성 자체만으로 SEO 점수를 높이는데 영향을 주지는 않는다.

하지만, **검색엔진 로봇의 일반적인 크롤링 과정에서
발견되지 않는 웹페이지에 대한 정보를 제공**함으로써
**더 많은 웹페이지가 크롤링되고 색인될 수 있게** 도와준다는 점에서
SEO에 긍정적인 영향을 끼친다고 볼 수 있다.

# 결론

## 구글의 권장사항

robots.txt에 **지정가능한 파일 수는 정해져 있지 않지만,**
**파일 용량은 500 KB 가 넘지 않아야 한다**고 권고하고 있다.

sitemap.xm은
하나의 사이트맵 인덱스 파일에
**각각 최대 50,000 개의 사이트맵과 웹 페이지 주소**를 지정할 수 있지만,
**인덱스 파일의 압축 전 크기가 50 MB,
웹 페이지 주소는 압축 전 크기가 10 MB를 넘지 않아야** 한다고 권장한다.

---

마지막으로 알아 두면 좋은 내용

> **구글의 경우,
> 사이트맵 인덱스 파일이
> 다른 사이트맵 인덱스 파일을 포함하면 에러가 발생한다.**

---

따라서 아래와 같은 사용을 권장한다.

### 권장 사용법

- robots.txt 파일에 사이트맵 또는 사이트맵 인덱스 파일을 수십 개 정도 두는 것은 전혀 문제 되지 않는다. 자동과 수동 등 생성 시기와 컨텐츠 구분에 따라 적당한 개수로 필요한 만큼 정의한다.
- 사이트맵 인덱스 파일과 사이트맵 파일에는 각각 최대 50,000 개의 사이트맵 또는 페이지 주소를 포함할 수 있지만, 파일 크기 제한 등을 고려하면 **10,000 개는 넘지 않도록 관리하는 것이 좋다.**

#### 설정 예

만약, robots.txt 파일에 3개의 사이트맵 인덱스 파일을 지정하고 10,000 개의 사이트맵과 10,000 개의 주소를 지정한다고 하면
총 3 x 10,000 x 10,000 = 3억 개의 웹 페이지 색인용 주소를 지정할 수 있다.

<BR/>

출처: [Robots.txt와 Sitemap.xml 알아보기](https://www.twinword.co.kr/blog/basic-technical-seo/)
